<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Primary Meta Tags -->
    <title>Domain Expertise as Executable Configuration: Why Knowledge That Runs Beats Docs That Rot | Building in Public</title>
    <meta name="title" content="Domain Expertise as Executable Configuration: Why Knowledge That Runs Beats Docs That Rot">
    <meta name="description" content="How I accidentally discovered that teaching AI assistants your expertise keeps knowledge fresh better than wikis ever could—with real examples from deployment verification, UX design, and tool expertise.">
    <meta name="keywords" content="knowledge management, Claude Code skills, AI assistants, documentation, executable knowledge, deployment verification, UX patterns">
    <meta name="author" content="Neel Ketkar">
    <link rel="canonical" href="https://sparrowfm.github.io/sparrow/posts/domain-expertise-as-code.html">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="alternate icon" href="../favicon.ico">

    <!-- RSS Feed -->
    <link rel="alternate" type="application/rss+xml" title="Building in Public RSS Feed" href="https://sparrowfm.github.io/sparrow/feed.xml">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://sparrowfm.github.io/sparrow/posts/domain-expertise-as-code.html">
    <meta property="og:title" content="Domain Expertise as Executable Configuration">
    <meta property="og:description" content="How teaching AI assistants your expertise keeps knowledge fresh better than wikis ever could.">
    <meta property="og:image" content="https://sparrowfm.github.io/sparrow/linkedin-blog-image.png">
    <meta property="og:image:secure_url" content="https://sparrowfm.github.io/sparrow/linkedin-blog-image.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:alt" content="Domain Expertise as Executable Configuration">
    <meta property="og:site_name" content="Building in Public">
    <meta property="article:published_time" content="2025-11-06T00:00:00Z">
    <meta property="article:author" content="Neel Ketkar">
    <meta property="article:tag" content="Knowledge Management">
    <meta property="article:tag" content="AI Assistants">
    <meta property="article:tag" content="Claude Code">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://sparrowfm.github.io/sparrow/posts/domain-expertise-as-code.html">
    <meta name="twitter:title" content="Domain Expertise as Executable Configuration">
    <meta name="twitter:description" content="How teaching AI assistants your expertise keeps knowledge fresh better than wikis ever could.">
    <meta name="twitter:image" content="https://sparrowfm.github.io/sparrow/linkedin-blog-image.png">
    <meta name="twitter:image:alt" content="Domain Expertise as Executable Configuration">

    <!-- Structured Data / JSON-LD -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Domain Expertise as Executable Configuration: Why Knowledge That Runs Beats Docs That Rot",
      "description": "How I accidentally discovered that teaching AI assistants your expertise keeps knowledge fresh better than wikis ever could.",
      "image": {
        "@type": "ImageObject",
        "url": "https://sparrowfm.github.io/sparrow/linkedin-blog-image.png",
        "width": 1200,
        "height": 630
      },
      "datePublished": "2025-11-06T00:00:00Z",
      "dateModified": "2025-11-06T00:00:00Z",
      "author": {
        "@type": "Person",
        "name": "Neel Ketkar",
        "url": "https://linkedin.com/in/ketkar",
        "sameAs": [
          "https://linkedin.com/in/ketkar",
          "https://github.com/sparrowfm"
        ]
      },
      "publisher": {
        "@type": "Person",
        "name": "Neel Ketkar"
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://sparrowfm.github.io/sparrow/posts/domain-expertise-as-code.html"
      },
      "keywords": ["knowledge management", "Claude Code", "AI assistants", "executable knowledge"],
      "articleBody": "How teaching AI assistants your expertise through structured configurations keeps knowledge fresh better than traditional documentation.",
      "inLanguage": "en-US"
    }
    </script>

    <!-- Stylesheets -->
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header class="site-header">
        <div class="container">
            <div class="header-content">
                <h1 class="site-title">Building in Public</h1>
                <p class="site-tagline">A non-technical founder building audio & media tech with AI</p>
            </div>
        </div>
    </header>

    <main class="container">
        <a href="../index.html" class="back-link">← Back to all posts</a>

        <article class="post-content">
            <h1>Domain Expertise as Executable Configuration: Why Knowledge That Runs Beats Docs That Rot</h1>

            <p><em>How I accidentally discovered that teaching AI assistants your expertise keeps knowledge fresh better than wikis ever could</em></p>

            <div class="post-meta">
                <time datetime="2025-11-06">November 6, 2025</time>
                <span class="post-category">Knowledge Management</span>
                <span class="post-category">AI Strategy</span>
                <span class="post-category">Claude Code</span>
            </div>

            <hr>

            <p><strong>About Me</strong>: I'm a business and product executive with zero coding experience. I've spent my career building products by working with engineering teams at <a href="https://www.amazon.com" target="_blank" rel="noopener">Amazon</a>, <a href="https://web.archive.org/web/20250101000000/wondery.com" target="_blank" rel="noopener">Wondery</a>, <a href="https://web.archive.org/web/20150101000000/fox.com" target="_blank" rel="noopener">Fox</a>, <a href="https://web.archive.org/web/20120101000000/rovicorp.com" target="_blank" rel="noopener">Rovi</a>, and <a href="https://web.archive.org/web/20060101000000/tvguide.com" target="_blank" rel="noopener">TV Guide</a>, but never wrote production code myself. Until recently.</p>

            <p>Frustrated with the pace of traditional development and inspired by the AI coding revolution, I decided to build my own projects using AI assistants (primarily Claude Code, Codex, and Cursor). This blog post is part of that journey—documenting what I've learned building real production systems as a complete beginner.</p>

            <hr>

            <h2>TL;DR</h2>

            <p>Configuration that executes in the context of work stays fresh. Documentation that sits unused rots. I accidentally discovered this building Claude Code "skills"—structured knowledge that gets actively applied while coding instead of passively referenced.</p>

            <p><strong>Key Learnings</strong>:</p>
            <ul>
                <li>Knowledge that gets used daily can't go stale (breaks immediately when wrong)</li>
                <li>"Skills" aren't code—they're structured expertise in markdown</li>
                <li>Active application beats passive reference for knowledge retention</li>
                <li>Pattern applies beyond AI assistants: linters, validators, code review bots</li>
            </ul>

            <hr>

            <h2>The Problem I Didn't Know I Had</h2>

            <p>When you're a solo founder building production systems with AI assistants, you face a weird version of the "knowledge transfer" problem: <strong>you're constantly context-switching between domains</strong>.</p>

            <p>One hour I'm debugging AWS deployments. Next hour I'm designing UX for creative tools. Then I'm analyzing calendar availability patterns.</p>

            <p>Traditional approach? Write docs. Keep a wiki. Reference it when you need to remember.</p>

            <p><strong>Reality?</strong> I never read my own docs. They go stale immediately. And when I come back to a domain after 2 weeks, I've forgotten the edge cases.</p>

            <p>Then I discovered something counterintuitive: <strong>The best way to capture domain expertise isn't documentation. It's configuration that executes.</strong></p>

            <hr>

            <h2>What I Built (And Why It Worked)</h2>

            <p>Over the past month, I've built three "skills" for Claude Code—not code, but <strong>structured knowledge configurations</strong> that Claude uses when helping me work. Think of them as "here's what an expert in X would know" captured in markdown.</p>

            <h3>Example 1: Deployment Verification (My Pain Point)</h3>

            <p><strong>The problem:</strong> I'd run <code>cdk deploy</code>, see "✅ Stack updated", and think I was done. Then discover the Lambda was in Failed state. Or the ECS task was stuck in PENDING. Or the image pushed to ECR but was 0 bytes.</p>

            <p><strong>Traditional solution:</strong> Write a deployment checklist wiki page.</p>
            <ul>
                <li>Problem: Never actually checked it</li>
                <li>Got out of date as AWS services evolved</li>
                <li>Didn't force me to verify before declaring victory</li>
            </ul>

            <p><strong>What I built instead:</strong> A Claude Code skill that knows deployment verification.</p>

            <p>It's not code—it's <strong>knowledge about what to check</strong>:</p>

            <pre><code># Deployment Verifier Skill

CRITICAL RULE: NEVER claim deployment success without verification.

For Lambda deployments, check:
- Function state is Active (not Pending/Failed)
- LastUpdateStatus is Successful
- Code was recently modified (timestamp matches deployment)
- No errors in CloudWatch logs
- Event sources are Enabled

For ECR image pushes, check:
- Image with specified tag exists
- Image was pushed recently (within expected timeframe)
- Image size is reasonable (not 0 bytes)
- Image manifest is valid

...</code></pre>

            <p><strong>Result:</strong> Now when I deploy, Claude <em>actively stops me</em> from saying "deployed successfully" until we've verified actual resource states. It's like having a paranoid DevOps engineer pair-programming with me.</p>

            <p><strong>Key insight:</strong> This "documentation" gets used <em>every single deployment</em>. It can't go stale because Claude is applying it in real-time. When AWS changes something, I update the skill once and it applies everywhere.</p>

            <h3>Example 2: Creative Tools UX (Knowledge I Keep Relearning)</h3>

            <p><strong>The problem:</strong> I'm building creative software (audio tools, media production). I have opinions about what makes tools usable vs frustrating, but I keep reinventing the wheel.</p>

            <p><strong>Traditional solution:</strong> "UX Best Practices" document with screenshots.</p>
            <ul>
                <li>Problem: Too general, or too specific</li>
                <li>Examples get outdated as tools evolve</li>
                <li>I'd forget to reference it during design</li>
            </ul>

            <p><strong>What I built instead:</strong> UX expertise as a Claude Code skill.</p>

            <p>Again, not code—<strong>knowledge about patterns</strong>:</p>

            <pre><code># Creative Tools UX Expert Skill

When designing features for creative tools, check for:

Non-Destructive Workflows:
✅ GOOD: Adjustment layers, version history, undo at any point
❌ BAD: Overwriting originals, no undo, destructive operations
Example: Photoshop Smart Objects (non-destructive), old batch processors (destructive)

Real-Time Preview:
✅ GOOD: Live preview, scrubbing, before/after toggles
❌ BAD: Long processing with no preview, blocking UI
Example: ElevenLabs voice preview (instant), old TTS tools (batch-only)

Parameter Control vs Black-Box:
✅ GOOD: Manual override, customizable presets, visible parameters
❌ BAD: "Magic" buttons with no tuning, all-or-nothing
Example: iZotope RX "Learn" mode (suggests + allows tuning), some AI tools (no control)

...</code></pre>

            <p><strong>Result:</strong> When I design new features, Claude actively checks them against these patterns <em>while I'm designing</em>. "Hey, this feature has no preview—users will hate that. Here's how ElevenLabs solved it..."</p>

            <p><strong>Key insight:</strong> The knowledge gets used during every design review. It stays current because I'm constantly refining it based on what actually matters in my workflow.</p>

            <h3>Example 3: Tool Usage Expertise (Using New Utility Tools)</h3>

            <p><strong>The context:</strong> A former colleague is building <a href="https://github.com/port42/port42" target="_blank" rel="noopener">Port42</a>, a brand-new command-line tool ecosystem with a cool vision: "consciousness computing" where intent becomes executable through conversation. It's still in early development, but the concept is fascinating—your tools evolve with your thinking patterns.</p>

            <p><strong>The problem:</strong> I started using Port42's <code>md-to-docx</code> tool frequently (converts Markdown to Word documents). Later, I'm trying <code>cal-avails</code> for calendar availability analysis. Both have dozens of options, edge cases, and usage patterns I keep forgetting.</p>

            <p><strong>Traditional solution:</strong> README files with examples.</p>
            <ul>
                <li>Problem: I'd forget which flags to use</li>
                <li>Edge cases not well documented</li>
                <li>Usage patterns scattered</li>
            </ul>

            <p><strong>What I'm building:</strong> Skills that know how to use these tools expertly.</p>

            <pre><code># Markdown to DOCX Converter Skill

When converting Markdown to Word documents:

Common usage patterns:
- Single file: md-to-docx document.md
- Batch conversion: md-to-docx *.md
- Preserve formatting: headings, bold, italic, lists, code blocks, tables

Best practices:
- Use --output/-o to organize output files
- Check for images and links before conversion
- Preview complex tables (may need manual adjustment)

Common gotchas:
- Custom markdown extensions may not convert
- Code blocks need proper language specification
- Image paths must be relative or absolute

# Calendar Availability Analyzer Skill (Experimental)

When analyzing calendar availability:

For finding meeting slots:
- Use --min-duration to filter by needed time
- Check both --work-hours and extended hours for flexibility
- Look for fragmentation patterns (many short slots = bad schedule)

Common mistakes:
- Forgetting timezone specification
- Not accounting for weekends when using date ranges
- Missing calendar source authentication

...</code></pre>

            <p><strong>Result:</strong> When I need to convert a blog post to Word format, Claude knows <code>md-to-docx</code> flags and gotchas. I use this constantly. The calendar analyzer is more experimental, but the same pattern applies—capture tool expertise once, apply it every time.</p>

            <hr>

            <h2>Why This Works Better Than Documentation</h2>

            <h3>1. Used Daily = Stays Current</h3>

            <p>Traditional docs rot because they're write-once, read-maybe.</p>

            <p>These skills get used <em>every time I work in that domain</em>. If something's wrong, I notice immediately and fix it. The knowledge evolves with my actual workflow.</p>

            <h3>2. Active Application vs Passive Reference</h3>

            <p>Docs require me to remember to check them.</p>

            <p>Skills actively guide me. Claude applies the knowledge proactively. It's like having domain experts pair-programming with you.</p>

            <h3>3. No "Code" to Maintain</h3>

            <p>This isn't code that can break or need refactoring. It's structured knowledge in markdown.</p>

            <p>Update is simple: Edit the markdown when you learn something new. No tests, no dependencies, no deployment.</p>

            <h3>4. Scales With Complexity</h3>

            <p>The more complex the domain, the more valuable this becomes.</p>

            <p>Deployment verification has 20+ checks across different AWS services. Creative tool UX has dozens of patterns. Calendar analysis has edge cases I'd never remember.</p>

            <p>All captured in ~10 pages of markdown per domain.</p>

            <hr>

            <h2>The Pattern: Executable Configuration</h2>

            <p>Here's what I learned: There's a category between "code" and "documentation" that's underutilized:</p>

            <p><strong>Configuration that executes in the context of work.</strong></p>

            <ul>
                <li>Not passive docs you might read</li>
                <li>Not code you have to maintain</li>
                <li>But knowledge that gets <em>actively applied</em> while you work</li>
            </ul>

            <p><strong>This works for:</strong></p>
            <ul>
                <li><strong>Verification checklists</strong> (deployment, quality, compliance)</li>
                <li><strong>Design patterns</strong> (UX, architecture, API conventions)</li>
                <li><strong>Tool expertise</strong> (complex CLIs, advanced features, edge cases)</li>
                <li><strong>Domain knowledge</strong> (industry-specific rules, best practices, gotchas)</li>
            </ul>

            <p><strong>It doesn't work for:</strong></p>
            <ul>
                <li>Strategic vision (too subjective)</li>
                <li>One-time processes (not worth the setup)</li>
                <li>Things that change constantly (maintenance > benefit)</li>
            </ul>

            <hr>

            <h2>How This Could Apply to Your Domain</h2>

            <p>After building these, I realized the pattern is broadly applicable:</p>

            <h3>Healthcare: Compliance Verification Patterns</h3>

            <p>Instead of a 200-page compliance manual, capture verification patterns:</p>
            <ul>
                <li>"Before submitting to FDA, verify these 15 formatting rules..."</li>
                <li>"HIPAA data checks: patient IDs must be hashed, dates ISO-8601..."</li>
                <li>Apply automatically via validation scripts or AI assistant checks</li>
            </ul>

            <h3>Finance: Calculation Verification Rules</h3>

            <p>Instead of tax manual, capture verification logic:</p>
            <ul>
                <li>"For CA stock options: holding period &lt; 365 days = short-term treatment..."</li>
                <li>"Reconciliation checks: totals must match within 0.01%, flag if variance &gt; $100..."</li>
                <li>Run automatically on every calculation</li>
            </ul>

            <h3>Legal: Process Knowledge</h3>

            <p>Instead of "how we file in X jurisdiction" docs:</p>
            <ul>
                <li>"Judge Martinez prefers Times New Roman 12pt, 1.5 spacing..."</li>
                <li>"Statutory deadlines: Federal = X, State = Y, local exceptions..."</li>
                <li>Validate before filing, auto-format to preferences</li>
            </ul>

            <h3>Engineering Teams: Architecture Patterns</h3>

            <p>Instead of architecture wiki:</p>
            <ul>
                <li>"API endpoints should follow REST conventions: GET for read, POST for create..."</li>
                <li>"Error handling: return structured JSON, include request_id, log with correlation_id..."</li>
                <li>Enforce via linters, code review assistants, or AI pair programmers</li>
            </ul>

            <hr>

            <h2>The Business Case</h2>

            <p>I'm a solo founder, so my "team scaling" is just me context-switching less painfully.</p>

            <p>But for actual teams, the math is compelling:</p>

            <p><strong>Traditional documentation approach:</strong></p>
            <ul>
                <li>Setup: 2-4 weeks to write</li>
                <li>Maintenance: Quarterly updates (rarely happens)</li>
                <li>Accuracy: 40-60% accurate after 12 months</li>
                <li>Onboarding: 2-6 weeks per new hire (reading docs, asking questions, making mistakes)</li>
            </ul>

            <p><strong>Executable configuration approach:</strong></p>
            <ul>
                <li>Setup: 1-3 weeks to capture initial patterns</li>
                <li>Maintenance: Continuous (forced by breaks when wrong)</li>
                <li>Accuracy: 95%+ (fails loudly when outdated)</li>
                <li>Onboarding: Days to weeks (learn by doing with guidance)</li>
            </ul>

            <p><strong>Break-even:</strong> For teams of 5+ people doing complex domain work, this typically pays for itself within 3-6 months through faster onboarding alone.</p>

            <p>Add in error reduction, consistency gains, and knowledge retention, and ROI is 3-5x in year one.</p>

            <hr>

            <h2>How to Start</h2>

            <p>If you're using AI assistants (Claude Code, Copilot, Cursor), start small:</p>

            <h3>1. Pick One Pain Point</h3>

            <p>What domain knowledge do you keep re-looking-up?</p>
            <ul>
                <li>Deployment verification steps?</li>
                <li>API design conventions?</li>
                <li>Testing patterns?</li>
                <li>Design review criteria?</li>
            </ul>

            <h3>2. Capture Core Patterns</h3>

            <p>Write 1-2 pages of markdown:</p>
            <ul>
                <li>What does an expert in this domain know?</li>
                <li>What are common mistakes?</li>
                <li>What are edge cases that bite you?</li>
                <li>What examples illustrate the patterns?</li>
            </ul>

            <h3>3. Configure Your AI Assistant</h3>

            <ul>
                <li><strong>Claude Code</strong>: Create a skill in <code>~/.claude/skills/</code></li>
                <li><strong>GitHub Copilot</strong>: Add custom instructions to <code>.github/copilot-instructions.md</code></li>
                <li><strong>Cursor</strong>: Add to project rules in <code>.cursorrules</code></li>
            </ul>

            <h3>4. Use It Daily</h3>

            <p>The magic happens when you <em>actually use it</em>. Don't just write it and forget.</p>

            <p>Every time you work in that domain, let the AI assistant apply the patterns. Refine based on what works.</p>

            <hr>

            <h2>What I'm Building Next</h2>

            <p>I'm thinking about skills for:</p>
            <ul>
                <li><strong>API design patterns</strong> (REST conventions, versioning, error handling)</li>
                <li><strong>Audio production patterns</strong> (FFmpeg workflows, format considerations)</li>
                <li><strong>Cost optimization checks</strong> (AWS resource sizing, identifying waste)</li>
            </ul>

            <p>Each one captures expertise I've built up but keep forgetting when I context-switch.</p>

            <hr>

            <h2>The Meta-Point</h2>

            <p>This blog post is about capturing domain expertise in executable forms.</p>

            <p>But the real insight is simpler: <strong>Knowledge that gets used stays accurate. Knowledge that sits unused rots.</strong></p>

            <p>The form doesn't matter as much as the frequency of application.</p>

            <ul>
                <li>Validation scripts run on every commit</li>
                <li>AI assistant configurations guide every coding session</li>
                <li>Linters enforce on every file save</li>
                <li>Code review bots check every PR</li>
            </ul>

            <p>All of these keep knowledge fresh because they <em>can't be ignored</em>.</p>

            <p>Traditional documentation can be ignored. That's why it rots.</p>

            <hr>

            <h2>Try It Yourself</h2>

            <p>I open-sourced my skills at: <a href="https://github.com/sparrowfm/claude-skills" target="_blank" rel="noopener">github.com/sparrowfm/claude-skills</a></p>

            <p>Start with <code>creative-tools-ux-expert</code> if you're building creative software, or use it as a template for your own domain.</p>

            <p>The setup is dead simple: Create a markdown file in <code>~/.claude/skills/your-skill-name/SKILL.md</code> with your domain knowledge. Claude Code loads it automatically.</p>

            <p>Not using Claude Code? The pattern still applies:</p>
            <ul>
                <li>Write validation scripts for compliance domains</li>
                <li>Configure AI coding assistants with your conventions</li>
                <li>Build linters that enforce your patterns</li>
                <li>Create code review bots with domain expertise</li>
            </ul>

            <p><strong>The key is: make knowledge executable, not just readable.</strong></p>

            <hr>

            <p><strong>What domain expertise are you constantly re-looking-up?</strong> I'd love to hear what knowledge you'd capture if you could make it executable. <a href="https://linkedin.com/in/ketkar" target="_blank" rel="noopener">Connect on LinkedIn</a> or open an issue on <a href="https://github.com/sparrowfm/claude-skills" target="_blank" rel="noopener">GitHub</a>.</p>

        </article>
    </main>

    <footer class="site-footer">
        <div class="container">
            <div class="footer-content">
                <p>&copy; 2025 Neel Ketkar. All rights reserved.</p>
                <div class="footer-links">
                    <a href="../about.html">About</a>
                    <a href="https://linkedin.com/in/ketkar" target="_blank" rel="noopener">LinkedIn</a>
                    <a href="https://github.com/sparrowfm" target="_blank" rel="noopener">GitHub</a>
                    <a href="../feed.xml" target="_blank" rel="noopener" title="Subscribe via RSS">
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" style="vertical-align: middle; margin-right: 4px;">
                            <rect width="24" height="24" rx="4" fill="#FF6600"/>
                            <circle cx="6" cy="18" r="2" fill="white"/>
                            <path d="M4 11C9.52 11 14 15.48 14 21" stroke="white" stroke-width="2.5" stroke-linecap="round" fill="none"/>
                            <path d="M4 4C13.94 4 22 12.06 22 22" stroke="white" stroke-width="2.5" stroke-linecap="round" fill="none"/>
                        </svg>
                        RSS
                    </a>
                </div>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>

    <!-- Analytics -->
    <script data-goatcounter="https://sparrowfm.goatcounter.com/count"
            async src="//gc.zgo.at/count.js"></script>
</body>
</html>
