<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Primary Meta Tags -->
    <title>Building a Semantic Search Engine for Music Cues | Building in Public</title>
    <meta name="title" content="Building a Semantic Search Engine for Music Cues: Local-First AI Without the API Costs">
    <meta name="description" content="How I'm planning a natural language search system for hundreds of music cues using local LLMs, audio analysis, and semantic embeddings—without spending a penny on API costs.">
    <meta name="keywords" content="semantic search, music search, local LLM, Ollama, sentence-transformers, audio analysis, librosa, AI coding, building in public, music production">
    <meta name="author" content="Neel Ketkar">
    <link rel="canonical" href="https://sparrowfm.github.io/sparrow/posts/building-local-music-cue-search.html">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="alternate icon" href="../favicon.ico">

    <!-- RSS Feed -->
    <link rel="alternate" type="application/rss+xml" title="Building in Public RSS Feed" href="https://sparrowfm.github.io/sparrow/feed.xml">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://sparrowfm.github.io/sparrow/posts/building-local-music-cue-search.html">
    <meta property="og:title" content="Building a Semantic Search Engine for Music Cues">
    <meta property="og:description" content="Planning a natural language search system for music cues using local LLMs and audio analysis—zero API costs, complete privacy.">
    <meta property="og:image" content="https://sparrowfm.github.io/sparrow/posts/music-cue-search-og-image.png">
    <meta property="og:image:secure_url" content="https://sparrowfm.github.io/sparrow/posts/music-cue-search-og-image.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:alt" content="Music cue search system architecture">
    <meta property="og:site_name" content="Building in Public">
    <meta property="article:published_time" content="2025-10-22T00:00:00Z">
    <meta property="article:author" content="Neel Ketkar">
    <meta property="article:tag" content="AI/ML">
    <meta property="article:tag" content="Audio Processing">
    <meta property="article:tag" content="Local LLM">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://sparrowfm.github.io/sparrow/posts/building-local-music-cue-search.html">
    <meta name="twitter:title" content="Building a Semantic Search Engine for Music Cues">
    <meta name="twitter:description" content="Natural language music search with local LLMs, audio analysis, and semantic embeddings—zero API costs.">
    <meta name="twitter:image" content="https://sparrowfm.github.io/sparrow/posts/music-cue-search-og-image.png">
    <meta name="twitter:image:alt" content="Music cue search system architecture">

    <!-- Structured Data / JSON-LD -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Building a Semantic Search Engine for Music Cues: Local-First AI Without the API Costs",
      "description": "Planning and designing a natural language search system for music cues using local LLMs, audio feature extraction, and semantic embeddings.",
      "image": {
        "@type": "ImageObject",
        "url": "https://sparrowfm.github.io/sparrow/posts/music-cue-search-og-image.png",
        "width": 1200,
        "height": 630
      },
      "datePublished": "2025-10-22T00:00:00Z",
      "dateModified": "2025-10-22T00:00:00Z",
      "author": {
        "@type": "Person",
        "name": "Neel Ketkar",
        "url": "https://linkedin.com/in/ketkar",
        "sameAs": [
          "https://linkedin.com/in/ketkar",
          "https://github.com/sparrowfm"
        ]
      },
      "publisher": {
        "@type": "Person",
        "name": "Neel Ketkar"
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://sparrowfm.github.io/sparrow/posts/building-local-music-cue-search.html"
      },
      "keywords": ["semantic search", "music search", "local LLM", "Ollama", "audio analysis", "AI coding", "building in public"],
      "articleBody": "Planning a semantic search system for music cues using local-first AI tools including Ollama for audio description generation, sentence-transformers for embeddings, and librosa for audio feature extraction.",
      "inLanguage": "en-US"
    }
    </script>

    <!-- Stylesheets -->
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header class="site-header">
        <div class="container">
            <div class="header-content">
                <h1 class="site-title">Building in Public</h1>
                <p class="site-tagline">A non-technical founder building audio & media tech with AI</p>
            </div>
        </div>
    </header>

    <main class="container">
        <a href="../index.html" class="back-link">← Back to all posts</a>

        <article class="post-content">
            <h1>Building a Semantic Search Engine for Music Cues: Local-First AI Without the API Costs</h1>

            <p><em>When you have hundreds of music files with inconsistent metadata, natural language search becomes essential—here's how I'm planning to build it without spending a dollar on API costs</em></p>

            <div class="post-meta">
                <time datetime="2025-10-22">October 22, 2025</time>
                <span class="post-category">AI/ML</span>
                <span class="post-category">Audio Processing</span>
                <span class="post-category">Local LLM</span>
            </div>

            <hr>

            <p><strong>About Me</strong>: I'm a business and product executive with zero coding experience. I've spent my career building products by working with engineering teams at <a href="https://www.amazon.com" target="_blank" rel="noopener">Amazon</a>, <a href="https://web.archive.org/web/20250101000000/wondery.com" target="_blank" rel="noopener">Wondery</a>, <a href="https://web.archive.org/web/20150101000000/fox.com" target="_blank" rel="noopener">Fox</a>, <a href="https://web.archive.org/web/20120101000000/rovicorp.com" target="_blank" rel="noopener">Rovi</a>, and <a href="https://web.archive.org/web/20060101000000/tvguide.com" target="_blank" rel="noopener">TV Guide</a>, but never wrote production code myself. Until recently.</p>

            <p>Frustrated with the pace of traditional development and inspired by the AI coding revolution, I decided to build my own projects using AI assistants (primarily Claude Code, Codex, and Cursor). This blog post is part of that journey—documenting what I've learned building real production systems as a complete beginner.</p>

            <hr>

            <h2>TL;DR</h2>

            <p><strong>The Problem</strong>: Several hundred music cues scattered across directories, inconsistent metadata, and no good way to search for "uplifting orchestral with strings" or "dark and tense synth."</p>

            <p><strong>The Solution I'm Planning</strong>: A local-first semantic search system that:</p>
            <ul>
                <li>Extracts audio features using <code>librosa</code> (tempo, key, spectral characteristics)</li>
                <li>Generates rich descriptions using local LLMs via Ollama (Llama 3.2 or Mistral)</li>
                <li>Creates semantic embeddings with <code>sentence-transformers</code> (100% local, no API calls)</li>
                <li>Indexes everything in SQLite for fast similarity search</li>
                <li>Serves results through a simple FastAPI + web UI</li>
            </ul>

            <p><strong>Cost</strong>: $0 (everything runs locally)<br>
            <strong>Privacy</strong>: 100% (audio never leaves my machine)<br>
            <strong>Trade-off</strong>: Slower initial indexing (~30-60 sec per file vs 5-10 sec with cloud APIs)</p>

            <p><strong>Key Learnings</strong>:</p>
            <ul>
                <li>Local LLMs have reached "good enough" quality for many tasks</li>
                <li>Audio feature extraction can replace true audio understanding for music cues</li>
                <li>Sentence-transformers are surprisingly powerful for semantic search</li>
                <li>KISS principle: Start with simple features, resist over-engineering</li>
                <li>The best architecture is one you'll actually finish</li>
            </ul>

            <hr>

            <h2>The Problem: Music Cue Hell</h2>

            <p>I have several hundred music cues from various projects spread across my machine. They're in multiple formats (WAV, MP3, FLAC, AIFF), scattered across different directories, and the metadata quality is... inconsistent.</p>

            <p>Some files have decent metadata:</p>
            <pre><code>upbeat-guitar-loop-120bpm.wav
dark-ambient-drone.mp3
jazz-piano-solo-Cmajor.flac</code></pre>

            <p>Others are basically useless:</p>
            <pre><code>track_final_v3_FINAL.wav
bounce_2024_03_15.mp3
audio_export.wav</code></pre>

            <h3>What I Need to Search For</h3>

            <p>When I'm working on a project, I think in terms of:</p>
            <ul>
                <li><strong>Mood/emotion</strong>: "Something melancholic and reflective"</li>
                <li><strong>Genre/style</strong>: "80s synth vibes" or "orchestral with strings"</li>
                <li><strong>Instrumentation</strong>: "Solo piano" or "electric guitar with drums"</li>
                <li><strong>Use case</strong>: "Background music for an interview" or "uplifting transition music"</li>
            </ul>

            <p>Traditional file search (Spotlight, grep, whatever) completely fails at this. I need <strong>semantic search</strong>—search that understands <em>meaning</em>, not just filename matches.</p>

            <hr>

            <h2>The Initial Question: Cloud or Local?</h2>

            <p>When I started planning this, my first instinct was to use cloud APIs:</p>
            <ul>
                <li><strong>OpenAI Whisper</strong> for audio understanding</li>
                <li><strong>Claude or GPT-4</strong> for generating descriptions</li>
                <li><strong>OpenAI embeddings</strong> for semantic search</li>
            </ul>

            <p>But then I did the math.</p>

            <h3>Cost Analysis: Cloud vs Local</h3>

            <table>
                <thead>
                    <tr>
                        <th>Task</th>
                        <th>Cloud Approach</th>
                        <th>Estimated Cost (500 files)</th>
                        <th>Local Approach</th>
                        <th>Cost</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Audio Analysis</td>
                        <td>Claude Haiku w/ audio samples</td>
                        <td>$3-8</td>
                        <td>librosa feature extraction + local LLM</td>
                        <td>$0</td>
                    </tr>
                    <tr>
                        <td>Embeddings</td>
                        <td>OpenAI text-embedding-3-small</td>
                        <td>$1-2</td>
                        <td>sentence-transformers (local)</td>
                        <td>$0</td>
                    </tr>
                    <tr>
                        <td>Re-indexing</td>
                        <td>Same costs every time</td>
                        <td>$4-10 each time</td>
                        <td>Free unlimited re-runs</td>
                        <td>$0</td>
                    </tr>
                    <tr>
                        <td><strong>Total</strong></td>
                        <td>-</td>
                        <td><strong>$5-10 (one-time)</strong></td>
                        <td>-</td>
                        <td><strong>$0</strong></td>
                    </tr>
                </tbody>
            </table>

            <p>Okay, $5-10 isn't a lot. But here's what changed my mind:</p>

            <ol>
                <li><strong>Re-indexing flexibility</strong> - With cloud APIs, every re-index costs money. Want to tweak the prompts? That's another $5-10. Add 50 new files? More costs.</li>
                <li><strong>Privacy</strong> - Music cues might be from commercial projects. Uploading to cloud APIs feels sketchy.</li>
                <li><strong>Learning opportunity</strong> - I've never built with local LLMs or audio analysis libraries. This is a chance to learn.</li>
                <li><strong>Speed isn't critical</strong> - This is a one-time indexing job. If it takes 4 hours instead of 1 hour, so what?</li>
            </ol>

            <p><strong>Decision: Go local.</strong></p>

            <hr>

            <h2>The Architecture: Local-First Everything</h2>

            <h3>How Audio Search Actually Works</h3>

            <p>Here's the high-level flow:</p>

            <pre><code>1. SCAN FILES
   └─> Find all audio files in specified directories
   └─> Extract basic metadata (title, artist, genre, duration)

2. ANALYZE AUDIO
   └─> Extract features with librosa:
       • Tempo (BPM)
       • Key/pitch
       • Spectral features (brightness, rolloff)
       • MFCC (timbre characteristics)
       • Onset rate (rhythm complexity)
   └─> Convert features to text description
   └─> Use local LLM (Ollama) to synthesize into searchable description

3. GENERATE EMBEDDINGS
   └─> Combine all metadata + AI description into rich text
   └─> Generate embedding with sentence-transformers
   └─> Store in SQLite

4. SEARCH
   └─> Convert user query to embedding
   └─> Cosine similarity search against all cues
   └─> Return ranked results</code></pre>

            <h3>The Key Components</h3>

            <h4>1. Audio Feature Extraction: librosa</h4>

            <p><a href="https://librosa.org/" target="_blank" rel="noopener">librosa</a> is a Python library for music and audio analysis. It can extract tons of useful features without actually "listening" to the audio:</p>

            <pre><code>import librosa

# Load audio file
y, sr = librosa.load('upbeat-guitar.wav')

# Extract tempo
tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
# Result: 120 BPM

# Extract key/pitch
chroma = librosa.feature.chroma_cqt(y=y, sr=sr)
# Result: C major

# Extract spectral features
spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)
# Result: "bright" or "dark" sound

# Extract MFCCs (timbre)
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
# Result: timbral characteristics</code></pre>

            <p>From these features, I can generate text like:</p>

            <blockquote>
                <p>"Tempo: 120 BPM, Key: C major, Brightness: High, Timbre: Warm, Rhythm complexity: Moderate"</p>
            </blockquote>

            <h4>2. Description Generation: Ollama (Local LLM)</h4>

            <p><a href="https://ollama.com/" target="_blank" rel="noopener">Ollama</a> makes running local LLMs ridiculously easy. No Docker, no complicated setup—just install and run.</p>

            <p>The flow:</p>

            <pre><code># Extract features from librosa
features = analyze_audio('upbeat-guitar.wav')

# Generate prompt for LLM
prompt = f"""
Based on these audio features, generate a rich description for music search:

Filename: upbeat-guitar.wav
Existing metadata: Title: "Upbeat Guitar Loop", Genre: "Rock"
Tempo: {features['tempo']} BPM
Key: {features['key']}
Spectral brightness: {features['brightness']}
Timbre: {features['timbre']}
Rhythm complexity: {features['rhythm']}

Generate a concise description including:
- Genre/style
- Mood/emotion
- Instrumentation (inferred from timbre)
- Suggested use cases

Return JSON: {{"genre": "...", "mood": "...", "instruments": "...", "description": "...", "tags": [...]}}
"""

# Call Ollama API (runs locally)
import ollama
response = ollama.chat(model='llama3.2', messages=[
    {'role': 'user', 'content': prompt}
])

# Parse JSON response
metadata = json.loads(response['message']['content'])
# Result: {
#   "genre": "Rock, Indie",
#   "mood": "Uplifting, Energetic",
#   "instruments": "Electric guitar, drums, bass",
#   "description": "Bright and energetic rock loop with driving rhythm",
#   "tags": ["upbeat", "guitar", "rock", "energetic", "loop"]
# }</code></pre>

            <p><strong>Why this works</strong>: Even though the LLM hasn't "heard" the audio, the features from librosa give it enough context to generate meaningful descriptions.</p>

            <h4>3. Semantic Embeddings: sentence-transformers</h4>

            <p><a href="https://www.sbert.net/" target="_blank" rel="noopener">sentence-transformers</a> is a Python library that generates embeddings locally. No API calls, no costs.</p>

            <pre><code>from sentence_transformers import SentenceTransformer

# Load model (downloads once, then cached)
model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, good quality

# Combine all metadata into searchable text
searchable_text = f"""
Title: {metadata['title']}
Genre: {metadata['genre']}
Mood: {metadata['mood']}
Instruments: {metadata['instruments']}
Description: {metadata['description']}
Tags: {', '.join(metadata['tags'])}
"""

# Generate embedding
embedding = model.encode(searchable_text)
# Result: 384-dimensional vector

# Store in SQLite
db.execute("INSERT INTO cues (path, metadata, embedding) VALUES (?, ?, ?)",
           (file_path, json.dumps(metadata), embedding.tobytes()))</code></pre>

            <h4>4. Search: Cosine Similarity</h4>

            <p>When a user searches, convert their query to an embedding and find the most similar cues:</p>

            <pre><code># User query
query = "uplifting orchestral music with strings for a dramatic scene"

# Generate query embedding
query_embedding = model.encode(query)

# Load all embeddings from database
cues = db.execute("SELECT path, metadata, embedding FROM cues").fetchall()

# Calculate cosine similarity
from sklearn.metrics.pairwise import cosine_similarity
scores = []
for cue in cues:
    cue_embedding = np.frombuffer(cue['embedding'])
    similarity = cosine_similarity([query_embedding], [cue_embedding])[0][0]
    scores.append((cue['path'], cue['metadata'], similarity))

# Sort by similarity (highest first)
results = sorted(scores, key=lambda x: x[2], reverse=True)[:10]

# Return top 10 results</code></pre>

            <hr>

            <h2>The Tech Stack</h2>

            <table>
                <thead>
                    <tr>
                        <th>Component</th>
                        <th>Technology</th>
                        <th>Why</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Backend</strong></td>
                        <td>Python + FastAPI</td>
                        <td>Great ecosystem for ML/audio, FastAPI is simple and fast</td>
                    </tr>
                    <tr>
                        <td><strong>Audio Analysis</strong></td>
                        <td>librosa</td>
                        <td>Industry standard for music feature extraction</td>
                    </tr>
                    <tr>
                        <td><strong>LLM</strong></td>
                        <td>Ollama (Llama 3.2 or Mistral)</td>
                        <td>Easy local inference, good quality, free</td>
                    </tr>
                    <tr>
                        <td><strong>Embeddings</strong></td>
                        <td>sentence-transformers</td>
                        <td>Local, fast, no API costs</td>
                    </tr>
                    <tr>
                        <td><strong>Database</strong></td>
                        <td>SQLite</td>
                        <td>Zero config, perfect for local apps</td>
                    </tr>
                    <tr>
                        <td><strong>Frontend</strong></td>
                        <td>HTML/CSS/JS (vanilla)</td>
                        <td>Simple web UI with search box and audio player</td>
                    </tr>
                </tbody>
            </table>

            <h3>Dependencies</h3>

            <pre><code># requirements.txt
fastapi==0.104.1
uvicorn==0.24.0
librosa==0.10.1
ollama==0.1.6
sentence-transformers==2.2.2
scikit-learn==1.3.2
mutagen==1.47.0  # For metadata extraction</code></pre>

            <p><strong>Total disk space</strong>: ~5GB (mostly for sentence-transformers model cache and Ollama models)</p>

            <hr>

            <h2>The Implementation Plan (KISS/YAGNI)</h2>

            <p>I'm planning to build this in phases, keeping it simple and only adding complexity when needed.</p>

            <h3>Phase 1: Audit & Extract (Est. 2 hours)</h3>

            <p><strong>Goal</strong>: Understand what I have.</p>

            <ul>
                <li>Recursively scan directories for audio files</li>
                <li>Extract existing metadata using <code>mutagen</code></li>
                <li>Use <code>ffprobe</code> for format info (duration, sample rate, bitrate)</li>
                <li>Generate CSV report: <code>path, title, artist, genre, duration, quality_score</code></li>
                <li>Identify which files need AI analysis (poor metadata)</li>
            </ul>

            <p><strong>Quality scoring</strong>:</p>
            <ul>
                <li><strong>Good</strong> (3 points): Has title, artist, genre, and description</li>
                <li><strong>Fair</strong> (2 points): Has some metadata but missing key fields</li>
                <li><strong>Poor</strong> (1 point): Filename only, no useful metadata</li>
            </ul>

            <h3>Phase 2: Audio Analysis (Est. 3 hours)</h3>

            <p><strong>Goal</strong>: Generate rich descriptions for all files.</p>

            <ul>
                <li>For files with "good" metadata: Skip AI analysis, just use existing data</li>
                <li>For "fair" and "poor" files: Run full analysis pipeline</li>
                <li>Extract audio features with librosa (30-second samples to save time)</li>
                <li>Generate descriptions using Ollama</li>
                <li>Store results in SQLite</li>
            </ul>

            <p><strong>Smart optimization</strong>: Only analyze the intro/middle/outro (3x 10-second clips) instead of the full file. This reduces processing time by ~70% while still capturing the essence of the music.</p>

            <h3>Phase 3: Indexing (Est. 1 hour)</h3>

            <p><strong>Goal</strong>: Generate embeddings for semantic search.</p>

            <ul>
                <li>Combine all metadata into searchable text</li>
                <li>Generate embeddings using sentence-transformers</li>
                <li>Store in SQLite with proper indexes</li>
                <li>Test search with sample queries</li>
            </ul>

            <h3>Phase 4: Search API (Est. 2 hours)</h3>

            <p><strong>Goal</strong>: Build a simple FastAPI backend.</p>

            <pre><code># main.py
from fastapi import FastAPI
from sentence_transformers import SentenceTransformer

app = FastAPI()
model = SentenceTransformer('all-MiniLM-L6-v2')

@app.post("/search")
async def search(query: str, limit: int = 10):
    # Generate query embedding
    query_embedding = model.encode(query)

    # Search database for similar cues
    results = search_similar(query_embedding, limit)

    return {"results": results}

@app.get("/cue/{id}")
async def get_cue(id: int):
    # Return full metadata for a specific cue
    return get_cue_by_id(id)

@app.get("/audio/{id}")
async def stream_audio(id: int):
    # Stream audio file for preview
    return FileResponse(get_audio_path(id))</code></pre>

            <h3>Phase 5: Web UI (Est. 3 hours)</h3>

            <p><strong>Goal</strong>: Simple, functional interface.</p>

            <ul>
                <li>Search box with example queries</li>
                <li>Real-time results (debounced, 300ms delay)</li>
                <li>Result cards showing:
                    <ul>
                        <li>Title, genre, mood, instruments</li>
                        <li>Description and tags</li>
                        <li>Similarity score (0-100%)</li>
                        <li>HTML5 audio player for preview</li>
                        <li>"Copy path" button</li>
                    </ul>
                </li>
                <li>Filters: duration range, genre, mood</li>
                <li>Keyboard shortcuts (↑/↓ to navigate results, Enter to play/pause)</li>
            </ul>

            <hr>

            <h2>Trade-offs: What I'm Giving Up</h2>

            <p>Going local-first means accepting some compromises:</p>

            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>Cloud APIs</th>
                        <th>Local-First</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Quality</strong></td>
                        <td>Best-in-class (GPT-4, Claude)</td>
                        <td>Good enough (Llama 3.2, Mistral)</td>
                    </tr>
                    <tr>
                        <td><strong>Speed</strong></td>
                        <td>5-10 sec per file</td>
                        <td>30-60 sec per file</td>
                    </tr>
                    <tr>
                        <td><strong>Setup</strong></td>
                        <td>API keys only</td>
                        <td>Install Ollama, download models (~5GB)</td>
                    </tr>
                    <tr>
                        <td><strong>Cost</strong></td>
                        <td>$5-10 per index</td>
                        <td>$0 forever</td>
                    </tr>
                    <tr>
                        <td><strong>Privacy</strong></td>
                        <td>Audio sent to APIs</td>
                        <td>100% local</td>
                    </tr>
                    <tr>
                        <td><strong>Re-indexing</strong></td>
                        <td>Costs every time</td>
                        <td>Free unlimited</td>
                    </tr>
                </tbody>
            </table>

            <p><strong>Is "good enough" quality acceptable?</strong></p>

            <p>For music cue search, yes. I'm not generating creative content or writing essays. I need:</p>
            <ul>
                <li>"This is orchestral music" → Local LLM can do this</li>
                <li>"Mood is uplifting and dramatic" → Features + LLM can infer this</li>
                <li>"Instruments include strings and piano" → Timbre analysis + LLM works</li>
            </ul>

            <p>The descriptions don't need to be <em>perfect</em>. They need to be <em>searchable</em>. Big difference.</p>

            <hr>

            <h2>What I'd Do Differently</h2>

            <h3>1. Start with a Smaller Test Set</h3>

            <p>Instead of indexing all 500 files at once, I should start with 50 representative samples. Test the pipeline, tune the prompts, validate search quality—<em>then</em> scale up.</p>

            <h3>2. Build the Search UI First</h3>

            <p>I'm planning to build the indexer first, then the search UI. But actually, I should build a <strong>mock search UI</strong> with fake data first. This helps me understand what metadata I actually need before spending hours extracting it.</p>

            <h3>3. Consider Hybrid Approach</h3>

            <p>Maybe use local LLMs for most files, but fall back to Claude API for files where local analysis struggles (e.g., very complex orchestral pieces). Best of both worlds: mostly free, occasionally high-quality.</p>

            <h3>4. Version the Embeddings</h3>

            <p>If I improve the prompts or switch models later, I'll need to re-generate embeddings. Should include a <code>version</code> field in the database so I can track which cues need re-indexing.</p>

            <hr>

            <h2>Key Learnings from the Planning Process</h2>

            <h3>1. KISS (Keep It Simple, Stupid) Is Hard</h3>

            <p>My first instinct was to over-engineer this:</p>
            <ul>
                <li>"Should I build a recommendation engine?"</li>
                <li>"What about playlist generation?"</li>
                <li>"Should I support collaborative filtering?"</li>
            </ul>

            <p><strong>No.</strong> I need to search music cues. That's it. Everything else is scope creep.</p>

            <h3>2. YAGNI (You Aren't Gonna Need It) Applies to Infrastructure Too</h3>

            <p>I almost convinced myself I needed:</p>
            <ul>
                <li>Kubernetes for deployment</li>
                <li>PostgreSQL instead of SQLite</li>
                <li>Redis for caching</li>
                <li>Docker for containerization</li>
            </ul>

            <p><strong>Why?</strong> This is a local tool for me. It doesn't need to scale to millions of users. SQLite + FastAPI running on <code>localhost:8000</code> is <em>plenty</em>.</p>

            <h3>3. The Best Architecture Is One You'll Finish</h3>

            <p>Cloud APIs would give me slightly better quality and faster indexing. But the local-first approach is more exciting to me because:</p>
            <ul>
                <li>I get to learn new tools (Ollama, librosa, sentence-transformers)</li>
                <li>I can experiment freely without worrying about API costs</li>
                <li>The entire system runs on my laptop with no dependencies</li>
            </ul>

            <p>If the architecture motivates me to finish the project, that's the right architecture.</p>

            <h3>4. Audio Feature Extraction Is Shockingly Powerful</h3>

            <p>I assumed I'd need to actually "listen" to the audio (like Whisper does for speech). But librosa can extract so much information just from the waveform:</p>
            <ul>
                <li>Tempo and rhythm</li>
                <li>Pitch and key</li>
                <li>Brightness and timbre</li>
                <li>Harmonic vs percussive content</li>
            </ul>

            <p>Combined with a local LLM, these features are enough to generate rich, searchable descriptions.</p>

            <hr>

            <h2>What's Next</h2>

            <p>This post is the <strong>planning phase</strong>. I haven't written a single line of code yet.</p>

            <p>Next steps:</p>
            <ol>
                <li><strong>Install dependencies</strong> - Ollama, librosa, sentence-transformers</li>
                <li><strong>Build the audit script</strong> - Scan directories, extract metadata, generate report</li>
                <li><strong>Test audio analysis</strong> - Run librosa + Ollama on 10 sample files, validate quality</li>
                <li><strong>Build the indexer</strong> - Full pipeline from audio → embeddings → SQLite</li>
                <li><strong>Build the search API</strong> - FastAPI backend with cosine similarity search</li>
                <li><strong>Build the web UI</strong> - Simple search interface with audio preview</li>
                <li><strong>Write a follow-up post</strong> - Document what actually happened vs what I planned</li>
            </ol>

            <p>I'm estimating 12-15 hours of work total. We'll see if that's optimistic or pessimistic.</p>

            <hr>

            <h2>Final Thoughts</h2>

            <p>This project is a great example of how AI coding tools change the calculus of what's worth building.</p>

            <p><strong>Before AI coding assistants</strong>:</p>
            <ul>
                <li>I'd need to learn librosa, Ollama, sentence-transformers, FastAPI, and SQLite</li>
                <li>Each library would take days or weeks to understand</li>
                <li>Total time to build: 2-3 months</li>
                <li>Likelihood of finishing: 20%</li>
            </ul>

            <p><strong>With AI coding assistants</strong>:</p>
            <ul>
                <li>I understand the <em>concepts</em> (embeddings, semantic search, audio features)</li>
                <li>Claude Code handles the implementation details</li>
                <li>Total time to build: 12-15 hours</li>
                <li>Likelihood of finishing: 90%</li>
            </ul>

            <p>The difference isn't that the AI "does it for me." The difference is that I can focus on <strong>what to build</strong> instead of <strong>how to implement every detail</strong>.</p>

            <p>I still need to:</p>
            <ul>
                <li>Understand the problem domain (music search, audio analysis)</li>
                <li>Choose the right architecture (local vs cloud, SQLite vs PostgreSQL)</li>
                <li>Make UX decisions (what metadata to show, how to display results)</li>
                <li>Validate quality (do the descriptions make sense? does search work?)</li>
            </ul>

            <p>But I don't need to memorize the librosa API or debug sentence-transformers tensor dimensions. That's what Claude Code is for.</p>

            <p><strong>The best part?</strong> Even if this project doesn't work perfectly, I'll have learned a ton about audio analysis, semantic search, and local LLMs. That knowledge carries forward to the next project.</p>

            <p>And that's the real value of building in public: the journey, not just the destination.</p>

            <hr>

            <p><em>Follow along as I build this. Next post will cover the actual implementation—what worked, what didn't, and what I learned along the way.</em></p>

        </article>
    </main>

    <footer class="site-footer">
        <div class="container">
            <div class="footer-content">
                <p>&copy; 2025 Neel Ketkar. All rights reserved.</p>
                <div class="footer-links">
                    <a href="../about.html">About</a>
                    <a href="https://linkedin.com/in/ketkar" target="_blank" rel="noopener">LinkedIn</a>
                    <a href="https://github.com/sparrowfm" target="_blank" rel="noopener">GitHub</a>
                    <a href="../feed.xml" target="_blank" rel="noopener" title="Subscribe via RSS">
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" style="vertical-align: middle; margin-right: 4px;">
                            <rect width="24" height="24" rx="4" fill="#FF6600"/>
                            <circle cx="6" cy="18" r="2" fill="white"/>
                            <path d="M4 11C9.52 11 14 15.48 14 21" stroke="white" stroke-width="2.5" stroke-linecap="round" fill="none"/>
                            <path d="M4 4C13.94 4 22 12.06 22 22" stroke="white" stroke-width="2.5" stroke-linecap="round" fill="none"/>
                        </svg>
                        RSS
                    </a>
                </div>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>

    <!-- Analytics -->
    <script data-goatcounter="https://sparrowfm.goatcounter.com/count"
            async src="//gc.zgo.at/count.js"></script>
</body>
</html>
